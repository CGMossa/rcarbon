
#' @title Monte-Carlo simulation test for SPDs 
#'
#' @description Global and local signficance test for comparing shapes of multiple SPDs using random permutations. 
#'
#' @param x A vector of radiocarbon ages 
#' @param errors A vector of errors corresponding to each radiocarbon age
#' @param nsim Number of simulations
#' @param bins A vector indicating which bin each radiocarbon date is assigned to.  
#' @param runm A number indicating the window size of the moving average to smooth the SPD. If set to \code{NA} no moving average is applied.Default is NA. 
#' @param timeRange  A vector of length 2 indicating the start and end date of the analysis in cal BP.
#' @param raw A logical variable indicating whether all permuted SPDs should be returned or not. Default is FALSE.
#' @param model A vector indicating the model to be fitted. One between \code{'exponential'}, \code{'explog'}, and \code{'custom'}. 
#' @param predgrid A data.frame containing calendar years (column \code{calBP} and associated summed probabilties (column \code{PrDens}). Required when \code{model} is set to \code{'custom'}.
#' @param calCurves A vector of calibration curves (one between 'intcal13','shcal13' and 'marine13'; default is 'intcal13')
#' @param datenormalised  Controls for calibrated dates with probability mass outside the timerange of analysis. If set to TRUE the total probability mass within the time-span of analysis is normalised to sum to unity. Notice that if set to TRUE, calibrations will be normalised (c.f. \code{\link{calibrate}}). Default is FALSE. 
#' @param spdnormalised A logical variable indicating whether the total probability mass of the SPD is normalised to sum to unity. 
#' @param ncores Number of cores used for for parallel execution. Default is 1.
#' @param fitonly A logical variable. If set to TRUE, only the the model fitting is executed. Default is FALSE. 
#' @param verbose A logical variable indicating whether extra information on progress should be reported. Default is TRUE.
#'
#' @details The function implements Timpson et al (2014) Monte-Carlo test for comparing a fitted statistical model to the observed SPD. The null hypothesis is that the observed data are generated by an exponential growth model (fitted using the \code{\link{nls}} function --- implemented with \code{model='exponential'} --- or by log-transforming the probability densities --- implemented with \code{model='explog'}) or by a user defined custom model (supplied as a two-columned data.frame). The fitted model is then 'uncalibrated' (see \code{\link{uncalibratei}}) and \emph{n} samples are randomly drawn in radiocarbon age, with \emph{n} equivalent to the number of bins. The simulated dates are then calibrated and a SPD is generated. This process is repeated \code{nsim} times, so that  a distribution of probabilities densities is assigned to each calendar year. The probabilites are then z-transformed, and a 95% simulation envelope is computed. Local signficance departures are defined as instances where the observed SPD (which is also z-transformed) is outside such envelope. A global signicance is also computed by comparing the total "area" outside the simulation envelope in the observed and simulated data. 
#'
#' @return An object of class \code{SpdModelTest} with the following elements
#' \itemize{
#' \item{\code{result}} {A four column data.frame containing the observed probability density (column \emph{PrDens}) and the lower and the upper values of the simulation envelope (columns \emph{lo} and \emph{hi}) for each calendar year column \emph{calBP}}
#' \item{\code{sim}} {A matrix containing the simulation results. Available only when \code{raw} is set to TRUE} 
#' \item{\code{pval}} {A numeric vector containing the p-value of the global signficance test.}  
#' \item{\code{fit}} {A data.frame containing the probability densities of the fitted model for each calendar year within the time range of analysis}  
#' \item{\code{coefficients}} {Coefficients of the fitted model. Available only when \code{model} is \code{'exponential'} or \code{'explog'}}  
#' }
#'
#' @references 
#' Timpson, A., Colledge, S., Crema, E., Edinborough, K., Kerig, T., Manning, K., Thomas, M.G., Shennan, S., (2014). Reconstructing regional population fluctuations in the European Neolithic using radiocarbon dates: a new case-study using an improved method. Journal of Archaeological Science 52, 549â€“557. doi:10.1016/j.jas.2014.08.011
#'
#'
#' @export



modelTest <- function(x, errors, nsim, bins=NA, runm=NA, timeRange=NA, raw=FALSE, model=c("exponential","explog","custom"), predgrid=NA, calCurves='intcal13', datenormalised=FALSE, spdnormalised=FALSE, ncores=1, fitonly=FALSE, verbose=TRUE){

    if (verbose){ print("Aggregating observed dates...") }
    if (is.na(bins[1])){
        samplesize <- nrow(x$metadata)
    } else {
        samplesize <- length(unique(bins))
    }

    perOut <- rangecheck(x=x,bins=bins,datenormalised=datenormalised,timeRange=timeRange)
    ## if (perOut>0)
    ## { 
    ##  warning(paste(round(perOut,2),"% of the bins have density 0 within the time range of analysis; the null model is likely to overestimate the summed density of dates",sep=""))
    ## }
    
    observed <- spd(x=x, bins=bins, timeRange=timeRange, datenormalised=datenormalised, runm=runm, spdnormalised=spdnormalised, verbose=FALSE)
    finalSPD <- observed$grid$PrDens
    if (fitonly == TRUE) {nsim <- 1}
    ## Simulation
    sim <- matrix(NA,nrow=length(finalSPD),ncol=nsim)
    if (verbose & !fitonly){
        print("Monte-Carlo test...")
    flush.console()
        pb <- txtProgressBar(min=1, max=nsim, style=3)
    }
    coeffs <- NA
    time <- seq(max(observed$grid$calBP),min(observed$grid$calBP),-1)
    if (model=="exponential"){
        plusoffset <- 0
        fit <- nls(y ~ exp(a + b * x), data=data.frame(x=time, y=finalSPD), start=list(a=0, b=0))
        est <- predict(fit, list(x=time))
        predgrid <- data.frame(calBP=time, PrDens=est)
    } else if (model=="explog"){
        plusoffset <- min(finalSPD[finalSPD!=0])/10000 
        finalSPD <- finalSPD+plusoffset #avoid log(0)
        fit <- lm(log(finalSPD)~observed$grid$calBP)
        coeffs <- fit$coefficients
        est <-  exp(coeffs[1]) * exp(time*coeffs[2])
        predgrid <- data.frame(calBP=time, PrDens=est)
    } else if (model=="custom"){
        if (length(predgrid)!=2){
            stop("If you choose a custom model, you must provide a proper predgrid argument (two-column data.frame of calBP and predicted densities).")
        }
        plusoffset <- 0
    } else {
        stop("Specified model not one of current choices.")
    }
    if (fitonly){
        print("Done (SPD and fitted model only).")
        res <- list(result=NA, sim=NA, pval=NA, osbSPD=observed, fit=predgrid, coefficients=coeffs)
        return(res)
    }
    cragrid <- uncalibrate(as.CalGrid(predgrid), calCurves=calCurves, verbose=FALSE)
    obscras <- x$metadata$CRA
    cragrid$PrDens[cragrid$CRA > max(obscras) | cragrid$CRA < min(obscras)] <- 0
    for (s in 1:nsim){
        if (verbose){ setTxtProgressBar(pb, s) }
        randomDates <- sample(cragrid$CRA, replace=TRUE, size=samplesize, prob=cragrid$PrDens)
        randomSDs <- sample(size=length(randomDates), errors, replace=TRUE)
        tmp <- calibrate(ages=randomDates,errors=randomSDs, timeRange=timeRange, calCurves=calCurves, normalised=datenormalised, ncores=ncores, verbose=FALSE, calMatrix=TRUE)
        simDateMatrix <- tmp$calmatrix
        sim[,s] <- apply(simDateMatrix,1,sum)
        sim[,s] <- sim[,s] + plusoffset
        ## sim[,s] <- (sim[,s]/sum(sim[,s])) * sum(predgrid$PrDens)
        if (spdnormalised){ sim[,s] <- (sim[,s]/sum(sim[,s])) }
        if (!is.na(runm)){
            sim[,s] <- runMean(sim[,s], runm, edge="fill")
        }
    }
    if (verbose){ close(pb) }
    # Envelope, z-scores, global p-value
    lo <- apply(sim,1,quantile,prob=0.025)
    hi <- apply(sim,1,quantile,prob=0.975)
    Zsim <- t(apply(sim,1,scale))
    zLo <- apply(Zsim,1,quantile,prob=0.025,na.rm=TRUE)
    zHi <- apply(Zsim,1,quantile,prob=0.975,na.rm=TRUE)
    Zscore_empirical <- (finalSPD - apply(sim, 1, mean))/apply(sim, 1, sd)
    busts <- which(Zscore_empirical< zLo)
    booms <- which(Zscore_empirical> zHi)
    busts2 <- which(finalSPD< lo)
    booms2 <- which(finalSPD> hi)
    observedStatistic <- sum(c(zLo[busts] - Zscore_empirical[busts]),c(Zscore_empirical[booms]-zHi[booms]))
    expectedstatistic <- abs(apply(Zsim,2,function(x,y){a=x-y;i=which(a<0);return(sum(a[i]))},y=zLo)) + apply(Zsim,2,function(x,y){a=x-y;i=which(a>0);return(sum(a[i]))},y=zHi)
    pvalue <- 1 - c(length(expectedstatistic[expectedstatistic <= observedStatistic]))/c(length(expectedstatistic)+1)
    # Results
    result <- data.frame(calBP=observed$grid$calBP,PrDens=finalSPD,lo=lo,hi=hi)
    if(raw==FALSE){ sim <- NA }
    res <- list(result=result, sim=sim, pval=pvalue, fit=predgrid, coefficients=coeffs)
    class(res) <- "SpdModelTest"
    if (verbose){ print("Done.") }
    return(res)
}


#' @title  Random mark permutation test for SPDs
#'
#' @description Global and local signficance test for comparing shapes of multiple SPDs using random permutations. 
#'
#' @param x A \code{CalDates} class object containing the calibrated radiocarbon dates.
#' @param marks A numerical or character vector containing the marks associated to each radiocarbon date.
#' @param timeRange  A vector of length 2 indicating the start and end date of the analysis in cal BP.
#' @param bins  A vector indicating which bin each radiocarbon date is assigned to.  
#' @param nsim Number of random permutations 
#' @param runm A number indicating the window size of the moving average to smooth the SPD. If set to \code{NA} no moving average is applied.Default is NA.  
#' @param datenormalised Controls for calibrated dates with probability mass outside the timerange of analysis. If set to TRUE the total probability mass within the time-span of analysis is normalised to sum to unity. Should be set to FALSE when the parameter \code{normalised} in \code{\link{calibrate}} is set to FALSE. Default is FALSE. 
#' @param spdnormalised A logical variable indicating whether the total probability mass of the SPD is normalised to sum to unity. 
#' @param raw A logical variable indicating whether all permuted SPDs should be returned or not. Default is FALSE.
#' @param verbose A logical variable indicating whether extra information on progress should be reported. Default is TRUE.
#'
#' @details The function generates a distribution of expected SPDs by randomly shuffling the marks assigned to each \emph{bin} (see \code{\link{spd}} for details on binning). The resulting distribution of probabilities for each \emph{mark} (i.e. group of dates) for each calendar year is z-transformed, and a 95% simulation evnelope is computed. Local signficance departures are defined as instances where the observed SPD (which is also z-transformed) is outside such envelope. A global signicance is also computed by comparing the total "area" outside the simulation envelope in the observed and simulated data. 
#'
#' @return An object of class \code{SpdPermTest} with the following elements
#' \itemize{
#' \item{\code{observed}} {A list containing data.frames with the summed probability (column \emph{PrDens} for each calendar year (column \emph{calBP} for each mark/group}
#' \item{\code{envelope}} {A list containing matrices with the lower and upper bound values of the simulation envelope for each mark/group} 
#' \item{\code{pValueList}} {A list of p-value associated with each mark/group}  
#' }
#'
#' @references 
#' Crema, E.R., Habu, J., Kobayashi, K., Madella, M., (2016). Summed Probability Distribution of 14 C Dates Suggests Regional Divergences in the Population Dynamics of the Jomon Period in Eastern Japan. PLOS ONE 11, e0154809. doi:10.1371/journal.pone.0154809

#'
#' @export


permTest <- function(x, marks,  timeRange, nsim, bins=NA, runm=NA, datenormalised=FALSE, spdnormalised=FALSE, raw=FALSE, verbose=TRUE){

    if (is.na(bins[1])){ bins <- as.character(1:nrow(x$metadata)) }
    binNames <- unique(bins)
    calyears <- data.frame(calBP=seq(timeRange[1], timeRange[2],-1))
    binnedMatrix <- matrix(nrow=nrow(calyears), ncol=length(binNames))
    GroupList <- vector()
    if (verbose & length(binNames)>1){
        print("Summing observed groups...")
        flush.console()
        pb <- txtProgressBar(min=1, max=length(binNames), style=3)
    }
    caldateTR <- as.numeric(x$metadata[1,c("StartBP","EndBP")])
    caldateyears <- seq(caldateTR[1],caldateTR[2],-1)
    check <- caldateTR[1] >= timeRange[1] & caldateTR[2] <= timeRange[2]
    ## Observed SPDs
    for (b in 1:length(binNames)){
        if (verbose & length(binNames)>1){ setTxtProgressBar(pb, b) }
        index <- which(bins==binNames[b])
        if (length(x$calmatrix)>1){
            if (!check){
                stop("The time range of the calibrated dataset must be at least as large as the spd time range.")
            } else {
                tmp <- x$calmatrix[,index, drop=FALSE]
                if (datenormalised){
                    tmp <- apply(tmp,2,FUN=function(x) x/sum(x))
                }
                spdtmp <- rowSums(tmp)
                if (length(binNames)>1){
                    spdtmp <- spdtmp / length(index)
                }
                binnedMatrix[,b] <- spdtmp[caldateyears<=timeRange[1] & caldateyears>=timeRange[2]]
            }
        } else {
            slist <- x$grids[index]
            slist <- lapply(slist,FUN=function(x) merge(calyears,x, all.x=TRUE)) 
            slist <- rapply(slist, f=function(x) ifelse(is.na(x),0,x), how="replace")
            slist <- lapply(slist, FUN=function(x) x[with(x, order(-calBP)), ])
            tmp <- lapply(slist,`[`,2)
            if (datenormalised){   
                outofTR <- lapply(tmp,sum)==0 # date out of range
                tmpc <- tmp[!outofTR]
                if (length(tmpc)>0){
                    tmp <- lapply(tmpc,FUN=function(x) x/sum(x))
                }
            }
            if (length(binNames)>1){
                spdtmp <- Reduce("+", tmp) / length(index)
            } else {
                spdtmp <- Reduce("+", tmp)
            }
           binnedMatrix[,b] <- spdtmp[,1]
        }
        GroupList[b] <- marks[index][1]
    }
    if (verbose & length(binNames)>1){ close(pb) }
    observedSPD <- vector("list",length=length(unique(GroupList)))
    names(observedSPD) <- unique(GroupList)
    for (d in 1:length(unique(GroupList))){
        focus <- unique(GroupList)[d]
        index <- which(GroupList==focus)
        tmpSPD <- apply(binnedMatrix[,index,drop=FALSE], 1, sum)
        if (!is.na(runm)){
            tmpSPD <- runMean(tmpSPD, runm, edge="fill")
        }
        if (d==1){
            dall <- tmpSPD
        } else {
            dall <- dall+tmpSPD
        }
        if (spdnormalised){ tmpSPD <- tmpSPD / sum(tmpSPD) }
        observedSPD[[d]] <- data.frame(calBP=calyears, PrDens=tmpSPD)
    }
    ## Permutations
    simulatedSPD <- vector("list",length=length(unique(GroupList)))
    for (d in 1:length(unique(GroupList))){
        simulatedSPD[[d]] <- matrix(NA, nrow=nrow(calyears), ncol=nsim)
    }
    if (verbose){
        print("Permuting the groups...")
        flush.console()
        pb <- txtProgressBar(min=1, max=nsim, style=3)
    }
    for (s in 1:nsim){
        if (verbose){ setTxtProgressBar(pb, s) }
        simGroupList <- sample(GroupList)
        for (d in 1:length(unique(simGroupList))){
            focus <- unique(GroupList)[d]
            index <- which(simGroupList==focus)
            tmpSPD <- apply(binnedMatrix[,index,drop=FALSE],1,sum)
            if (!is.na(runm)){
                tmpSPD <- runMean(tmpSPD, runm, edge="fill")
            }
            if (d==1){
                dall <- tmpSPD
            } else {
                dall <- dall+tmpSPD
            }
            if (spdnormalised){ tmpSPD <- tmpSPD/sum(tmpSPD) }
            simulatedSPD[[d]][,s] <- tmpSPD
        }
    }
    names(simulatedSPD) <- unique(GroupList)
    if (verbose){ close(pb) }
    ## Statistics
    simulatedCIlist <- vector("list",length=length(unique(GroupList)))
    for (d in 1:length(unique(GroupList))){
        simulatedCIlist[[d]] <- cbind(apply(simulatedSPD[[d]],1,quantile,prob=c(0.025)), apply(simulatedSPD[[d]],1,quantile,prob=c(0.975)))
        names(simulatedCIlist) <- unique(GroupList)
    }
    pValueList <- numeric(length=length(simulatedSPD))
    for (a in 1:length(simulatedSPD)){
        zscoreMean <- apply(simulatedSPD[[a]],1,mean)
        zscoreSD <- apply(simulatedSPD[[a]],1,sd)
        tmp.sim <- t(apply(simulatedSPD[[a]],1,function(x){ return((x - mean(x))/sd(x)) }))
        tmp.sim[is.na(tmp.sim)] <- 0
        tmp.obs <- observedSPD[[a]]
        tmp.obs[,2] <- (tmp.obs[,2] - zscoreMean) / zscoreSD
        tmp.obs[is.na(tmp.obs[,2]),2] <- 0
        tmp.ci <- t(apply(tmp.sim,1, quantile, prob=c(0.025,0.975)))
        expectedstatistic <- abs(apply(tmp.sim,2,function(x,y){a=x-y;i=which(a<0);return(sum(a[i]))},y=tmp.ci[,1])) + apply(tmp.sim,2,function(x,y){a=x-y;i=which(a>0);return(sum(a[i]))},y=tmp.ci[,2])   
        lower <- tmp.obs[,2] - tmp.ci[,1]
        indexLow <- which(tmp.obs[,2] < tmp.ci[,1])
        higher <- tmp.obs[,2] - tmp.ci[,2]
        indexHi <- which(tmp.obs[,2] > tmp.ci[,2])
        observedStatistic <- sum(abs(lower[indexLow]))+sum(higher[indexHi])
        pValueList[[a]] <- 1
        if (observedStatistic>0){    
            pValueList[[a]] <- 1 - c(length(expectedstatistic[expectedstatistic <= observedStatistic]))/c(length(expectedstatistic)+1)
        }
        names(pValueList) <- unique(GroupList)
    }        
    res <- list(observed=observedSPD, envelope=simulatedCIlist, pValueList=pValueList)
    if (raw){ res$raw <- simulatedSPD }
    class(res) <- "SpdPermTest"
    if (verbose){ print("Done.") }
    return(res)
}



#' @title Spatial Permutation Test of summed probability distributions.
#
#' @description This function carries out local spatial permutation test of the summed probability distributions of radiocarbon dates for detecting local deviations in growth rates (Crema et al submitted). 
#' 
#' @param calDates  A \code{CalDates} class object.
#' @param timeRange A vector of length 2 indicating the start and end date of the analysis in cal BP
#' @param bins A vector indicating which bin each radiocarbon date is assigned to. Must have the same length as the number of radiocarbon dates. Can be created using the  \code{\link{binPrep}}) function. Bin names should follow the format "x_y", where x refers to a unique location (e.g. a site) and y is a integer value (e.g. "S023_1", "S023_2","S034_1", etc.).  
#' @param locations A \code{SpatialPoints} or a \code{SpatialPointsDataFrame} class object. Rownames of each point should much the first part of the bin names supplied (e.g. "S023","S034") 
#' @param breaks A vector of break points for defining the temporal slices.
#' @param spatialweights A \code{spatialweights} class object defining the spatial weights between the locations.
#' @param nsim The total number of simulations. Default is 1000.
#' @param runm The window size of the moving window average. Must be set to \code{NA} if a the rates of change area calculated from the raw SPDs. 
#' @param permute Indicates whether the permutations should be based on the \code{"bins"} or the \code{"locations"}. Default is \code{"locations"}. 
#' @param ncores Number of cores used for for parallel execution. Default is 1.
#' @param datenormalised a logical variable indicating whether the probability mass of each date within \code{timeRange} is equal to 1.Default is FALSE. 
#' @param verbose a logical variable indicating whether extra information on progress should be reported. Default is TRUE.

#'
#' @details The function consists of the following seven steps: 1) for each location (e.g. a site) generate a local SPD of radiocarbon dates weighting the contribution of dates from neighbouring site using a weight scheme provided by the \code{spatialweights} class object. 2) define temporal slices (using \code{breaks} as break values) the compute the total probability mass within each slice; 3) compute the rate of change between as abutting temporal slices by using the formula: \eqn{(SPD_{t}/SPD_{t+1}^{1/\Delta t}-1)}; 4) randomise the location of indivual bins or the entire sequence of bins associated with a given location and carry out steps 1--3; 5) repeate step 4 \code{nsim} times and generate, for each location, a distribution of growth rates under the null hypothesis (i.e. spatial independence); 6) compare, for each location, the observed growth rate to the distribution under the null hypothesis and compute the p-values; and 7) compute the false-discovery rate (i.e.q-value) for each location.    
#'
#' @return A \code{spatialTest} class object
#'
#' @references
#' Crema, E.R., Bevan, A., Shennan, S. (submitted). Spatio-temporal approaches to archaeological radiocarbon dates.
#' 
#' @seealso \code{\link{permTest}} for a non-spatial permutation test; \code{\link{plot.spatialTest}} for plotting
#' @export
 

SPpermTest<-function(calDates, timeRange, bins, locations, breaks, spatialweights, nsim=1000, runm=NA, verbose=TRUE,permute="locations",ncores=1,datenormalised=FALSE)
{

###################################
#### Load Dependency Libraries ####
###################################
    require(sp)	
    require(fdrtool)
    require(rcarbon)
    if (ncores>1) {require(doParallel)}

##################################
#### Initial warning messages ####
##################################

     if (!"CalDates" %in% class(calDates)){
        stop("calDates must be an object of class 'calDates'.")
    }
    if (length(bins)>1){
         if (any(is.na(bins))){
            stop("Cannot have NA values in bins.")
        }
        if (length(bins)!=length(calDates$grid)){
            stop("bins (if provided) must be the same length as x.")
        }
         } else {
        bins <- rep("0_0",length(calDates$grid))
    }

   if (!("SpatialPoints" %in% class(locations)[1]|"SpatialPointsDataFrame" %in% class(locations)[1])){
        stop("locations must be an object of class 'SpatialPoints' or 'SpatialPointsDataFrame'.")
    }

   locations.id=row.names(locations@coords)
    if (is.null(locations.id))
    {
        stop("locations must have rownames")
    }
    if (!all(range(timeRange)==range(breaks)))
    {
	stop("Range of breaks values must much match the temporal range defined by timeRange")
    }
   

#############################
#### Create binnedMatrix ####
#############################

    binNames <- unique(bins)
    calyears <- data.frame(calBP=seq(timeRange[1], timeRange[2],-1))
    binnedMatrix <- matrix(NA, nrow=nrow(calyears), ncol=length(binNames))


    if (verbose & length(binNames)>1){
        print("Binning by site/phase...")
        flush.console()
        pb <- txtProgressBar(min=1, max=length(binNames), style=3, title="Binning by site/phase...")
    }
    for (b in 1:length(binNames)){
        if (verbose & length(binNames)>1){ setTxtProgressBar(pb, b) }
        index <- which(bins==binNames[b])
        slist <- calDates$grid[index]
        slist <- lapply(slist,FUN=function(x) merge(calyears,x, all.x=TRUE)) 
        slist <- rapply(slist, f=function(x) ifelse(is.na(x),0,x), how="replace")
        slist <- lapply(slist, FUN=function(x) x[with(x, order(-calBP)), ])
        tmp <- lapply(slist,`[`,2)
        if (datenormalised){
            tmp <- lapply(tmp,FUN=function(x) x/sum(x))
        }
        if (length(binNames)>1){
            spd.tmp <- Reduce("+", tmp) / length(index)
        } else {
            spd.tmp <- Reduce("+", tmp)
        }
	binnedMatrix[,b] <- spd.tmp[,1]
    }
    if (verbose & length(binNames)>1){ close(pb) }


################################
### Observed Data Subroutine ###
################################ 


## Aggregate by Locations ##
    origins=unlist(lapply(strsplit(binNames,"_"),function(x){x[[1]]}))

    if (!all(origins%in%locations.id))
     {
        stop("Missing bins or locations")
     }

    resMatrix=matrix(NA,nrow=length(unique(locations.id)),ncol=nrow(binnedMatrix))
    
    for (x in 1:length(unique(locations.id))) 
        {
            index=which(origins==unique(locations.id)[x])
            if(length(index)>1)
                {resMatrix[x,]=apply(binnedMatrix[,index],1,sum)}
            if(length(index)==1)
                {resMatrix[x,]=binnedMatrix[,index]}
        }

 ## Aggregate by break s##

    nBreaks=length(breaks)-1
    obsMatrix=matrix(NA,nrow=length(unique(locations.id)),ncol=nBreaks)
    timeSequence=timeRange[1]:timeRange[2]
    
    for (x in 1:nBreaks)
        {
            index=which(timeSequence<=breaks[x]&timeSequence>breaks[x+1])
            obsMatrix[,x]=apply(resMatrix[,index],1,sum)
        }

## Apply SpatialWeights ##

    obsGridVal=t(spatialweights$w)%*%obsMatrix

## Compute Rate of Change #3

    rocaObs=t(apply(obsGridVal,1,function(x,d){
		   L=length(x)
		   res=numeric(length=L-1)
		   for (i in 1:c(L-1))
			{
			res[i]=(x[i+1]/x[i])^(1/d)-1
			}
		   return(res)},
		   d=abs(breaks[2]-breaks[1])))

##############################
### Permutation Subroutine ###
############################## 

    if (ncores>1)
   	 {	
          cl <- makeCluster(ncores)
          registerDoParallel(cl)
          print(paste("Running permutation test in parallel on ",getDoParWorkers()," workers...",sep=""))
	  sumcombine<-function(a,b)
		{
		list(a[[1]]+b[[1]],a[[2]]+b[[2]],a[[3]]+b[[3]])
		}
	  resultHiLoEq<-foreach (x=1:nsim,.combine= sumcombine) %dopar% {

            simGridVal<-matrix(NA,nrow=nrow(spatialweights$w),ncol=nBreaks)
            
	    ## Aggregate by Site ## 

            simResMatrix=matrix(0,nrow=length(unique(locations.id)),ncol=nrow(binnedMatrix))

            ## Randomly assigne bins to locations.id ##
           
	    if (permute=="bins")
	    {    
	    simOrigins=sample(origins)
            for (x in 1:length(unique(locations.id)))
                {                    
                    index=which(simOrigins==unique(locations.id)[x])
                    if(length(index)>1)
                        {simResMatrix[x,]=apply(binnedMatrix[,index],1,sum)}
                    if(length(index)==1)
                        {simResMatrix[x,]=binnedMatrix[,index]}
                }
            

            ## Aggregate by breaks ##
	    
            aggMatrix=matrix(NA,nrow=length(unique(locations.id)),ncol=nBreaks)
            
            for (x in 1:nBreaks)
                {
                    index=which(timeSequence<=breaks[x]&timeSequence>breaks[x+1])
                    aggMatrix[,x]=apply(simResMatrix[,index],1,sum)
                }
		       

           ## Apply Weights ##

           simGridVal=t(spatialweights$w)%*%aggMatrix
	    }
	    if (permute=="locations")
	    {
	     simMatrix=obsMatrix[sample(nrow(obsMatrix)),]	
             simGridVal=t(spatialweights$w)%*%simMatrix
		
	    }


           ## Compute Rate of Change ##

           rocaSim=t(apply(simGridVal,1,function(x,d){
		   L=length(x)
		   res=numeric(length=L-1)
		   for (i in 1:c(L-1))
			{
			res[i]=(x[i+1]/x[i])^(1/d)-1
			}
		   return(res)},
		   d=abs(breaks[2]-breaks[1])))

	    lo=rocaObs<rocaSim	    
	    hi=rocaObs>rocaSim
	    eq=rocaObs==rocaSim

          return(list(hi,lo,eq))
	  }
        stopCluster(cl)

        lo=resultHiLoEq[[1]]
	hi=resultHiLoEq[[2]]
	eq=resultHiLoEq[[3]]
    
	} else {

    hi=matrix(0,nrow=nrow(spatialweights$w),ncol=nBreaks-1)
    lo=matrix(0,nrow=nrow(spatialweights$w),ncol=nBreaks-1)
    eq=matrix(0,nrow=nrow(spatialweights$w),ncol=nBreaks-1)

    print("Permutation test...")
    flush.console()

    pb <- txtProgressBar(min = 1, max = nsim, style=3)

        for (s in 1:nsim)
        {
            setTxtProgressBar(pb, s)
	    simGridVal<-matrix(NA,nrow=nrow(spatialweights$w),ncol=nBreaks)
            ## Aggregate by Site ## 
            simResMatrix=matrix(0,nrow=length(unique(locations.id)),ncol=nrow(binnedMatrix))

            ## Randomly assign bins to locations
            if (permute=="bins")
	    {
	    simOrigins=sample(origins)
            



            for (x in 1:length(unique(locations.id)))
                {                    
                    index=which(simOrigins==unique(locations.id)[x])
                    if(length(index)>1)
                        {simResMatrix[x,]=apply(binnedMatrix[,index],1,sum)}
                    if(length(index)==1)
                        {simResMatrix[x,]=binnedMatrix[,index]}
                }
            

            ##Aggregate by breaks##
            aggMatrix=matrix(NA,nrow=length(unique(locations.id)),ncol=nBreaks)
            
            for (x in 1:nBreaks)
                {
                    index=which(timeSequence<=breaks[x]&timeSequence>breaks[x+1])
                    aggMatrix[,x]=apply(simResMatrix[,index],1,sum)
                }
		       

           ##Apply Weights 
           simGridVal=t(spatialweights$w)%*%aggMatrix
	    }
           if (permute=="locations")
           {
	     simMatrix=obsMatrix[sample(nrow(obsMatrix)),]	
             simGridVal=t(spatialweights$w)%*%simMatrix
	   }



           ##Compute Rate of Change
           rocaSim=t(apply(simGridVal,1,function(x,d){
		   L=length(x)
		   res=numeric(length=L-1)
		   for (i in 1:c(L-1))
			{
			res[i]=(x[i+1]/x[i])^(1/d)-1
			}
		   return(res)},
		   d=abs(breaks[2]-breaks[1])))

	    hi=hi+(rocaObs>rocaSim)
	    lo=lo+(rocaObs<rocaSim)
	    eq=eq+(rocaObs==rocaSim)
	    
        }
    close(pb)
    }


############################
### Compute Significance ###
############################ 
    
    pvalHi=(lo+eq+1)/c(nsim+1)
    pvalLo=(hi+eq+1)/c(nsim+1)
    pval=pvalHi
    pval[which(pvalHi>pvalLo)]=pvalLo[which(pvalHi>pvalLo)]
    pval=pval*2
    if (max(pval)>1)
    {
    	 pval[which(pval>1)]=1
    }

    ## Compute False Discovery Rate (q-value) ##
    
    qvalHi=apply(pvalHi,2,function(x){return(fdrtool(x,statistic="pvalue",plot=FALSE,verbose=FALSE)$qval)})
    qvalLo=apply(pvalLo,2,function(x){return(fdrtool(x,statistic="pvalue",plot=FALSE,verbose=FALSE)$qval)})
    qval=apply(pval,2,function(x){return(fdrtool(x,statistic="pvalue",plot=FALSE,verbose=FALSE)$qval)})

    metadata=data.frame(npoints=length(unique(locations.id)),ndates=nrow(calDates$metadata),nbins=length(binNames),nsim=nsim,permutationType=permute,datenormalised=datenormalised,breaks=nBreaks,timeRange=paste(timeRange[1],"-",timeRange[2],sep=""),weights.h=spatialweights$h,weights.kernel=spatialweights$kernel)
   
    reslist=list(metadata=metadata,rocaObs=rocaObs,pval=pval,pvalHi=pvalHi,pvalLo=pvalLo,qval=qval,qvalLo=qvalLo,qvalHi=qvalHi,locations=locations)
    
    class(reslist) <- append(class(reslist),"spatialTest")
    return(reslist)
}


